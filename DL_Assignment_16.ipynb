{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOA6TY7jOurxhge6+gyldYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maheshkumar145/DL_Theory/blob/main/DL_Assignment_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the Activation Functions in your own language**\n",
        "\n",
        "**a)\tsigmoid**\n",
        "\n",
        "This function takes any real value as input and outputs values in the range of 0 to 1. \n",
        "The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0\n",
        "\n",
        "**b)\ttanh**\n",
        "\n",
        "Tanh function is very similar to the sigmoid/logistic activation function, and even has the same S-shape with the difference in output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.\n",
        "\n",
        "**c)\tReLU**\n",
        "\n",
        "ReLU stands for Rectified Linear Unit. \n",
        "Although it gives an impression of a linear function, ReLU has a derivative function and allows for backpropagation while simultaneously making it computationally efficient. \n",
        "The neurons will only be deactivated if the output of the linear transformation is less than 0.\n",
        "\n",
        "**d)\tELU**\n",
        "\n",
        "Exponential Linear Unit, or ELU for short, is also a variant of ReLU that modifies the slope of the negative part of the function. \n",
        "ELU uses a log curve to define the negativ values unlike the leaky ReLU and Parametric ReLU functions with a straight line.\n",
        "\n",
        "**e)\tLeakyReLU**\n",
        "\n",
        "Leaky ReLU is an improved version of ReLU function to solve the Dying ReLU problem as it has a small positive slope in the negative area.\n",
        "\n",
        "**f)\tswish**\n",
        "\n",
        "This function is bounded below but unbounded above i.e. Y approaches to a constant value as X approaches negative infinity but Y approaches to infinity as X approaches infinity."
      ],
      "metadata": {
        "id": "lyyC_3L1Qoje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What happens when you increase or decrease the optimizer learning rate?**\n",
        "\n",
        "**Ans:** A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck"
      ],
      "metadata": {
        "id": "-gMbarSlQnK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What happens when you increase the number of internal hidden neurons?**\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "1) Increasing the number of hidden layers might improve the accuracy or might not, it really depends on the complexity of the problem that you are trying to solve.\n",
        "\n",
        "2) Increasing the number of hidden layers much more than the sufficient number of layers will cause accuracy in the test set to decrease, yes. It will cause your network to overfit to the training set, that is, it will learn the training data, but it won't be able to generalize to new unseen data"
      ],
      "metadata": {
        "id": "eGjoKMbJQl9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What happens when you increase the size of batch computation?**\n",
        "\n",
        "**Ans:** Increasing batch size reduces the learners capacity to generalize. Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima, tend to result in models that become caught in local minima."
      ],
      "metadata": {
        "id": "1P-PW1Q1QkyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Why we adopt regularization to avoid overfitting?**\n",
        "\n",
        "**Ans:** Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting."
      ],
      "metadata": {
        "id": "8qcjr1xpQjYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. What are loss and cost functions in deep learning?**\n",
        "\n",
        "**Ans:**  Loss function is to capture the difference between the actual and predicted values for a single record whereas cost functions aggregate the difference for the entire training dataset"
      ],
      "metadata": {
        "id": "0rPXPm9MQiJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. What do ou mean by underfitting in neural networks?**\n",
        "\n",
        "**Ans:** Underfitting is a scenario where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data."
      ],
      "metadata": {
        "id": "j5OU-vftQg0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Why we use Dropout in Neural Networks?**\n",
        "\n",
        "**Ans:** Dropout refers to data, or noise, that's intentionally dropped from a neural network to improve processing and time to results."
      ],
      "metadata": {
        "id": "u_yw0luBQeH2"
      }
    }
  ]
}